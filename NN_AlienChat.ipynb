{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting next word in a sentence \n",
    "## word here is represented by a character\n",
    "Dataset is not published here, but for an idea, training is done on sentences that look like <b>AEDF234TH</b> and testing is done on sentences like <b>ASE452DG?</b>.\n",
    "\n",
    "This is a standard Neural Network implementation of this problem, using [Keras](https://keras.io/).\n",
    "\n",
    "More details on Data preprocessiong and Dataset preparation can be found here in the [LSTM implementation](AlienChat_LSTM_Keras.ipynb) of the problem\n",
    "\n",
    "\n",
    "The best `train` accuracy of the model (with least `validation` loss,  achieved as the following : `\n",
    "\n",
    "| Total Hidden layers      | Neurons / Hidden Layer | Epochs     | Train (loss / accuracy)| Validation (loss / accuracy)| Test (loss / accuracy)|\n",
    "| :---        |    :----:   |    :----:   |    :----:   |    :----:   |    :----:   |\n",
    "| 2      | 100       | 60   | 0.8924 / **69.08%** | 2.4394 / 38.70% | 0.9421 / **70.18%*|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from alienchat import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"N\": 13, \"O\": 14, \"P\": 15, \"4\": 29, \"C\": 2, \"H\": 7, \"3\": 28, \"T\": 19, \"A\": 0, \"V\": 21, \"6\": 31, \"G\": 6, \"Q\": 16, \"K\": 10, \"I\": 8, \"1\": 26, \"R\": 17, \"S\": 18, \"Y\": 24, \"W\": 22, \"X\": 23, \"B\": 1, \"2\": 27, \"F\": 5, \"L\": 11, \"7\": 32, \"E\": 4, \"Z\": 25, \"M\": 12, \"J\": 9, \"D\": 3, \"5\": 30, \"U\": 20}'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Utils for this problem\n",
    "\n",
    "SENTENCE_LENGTH = 9\n",
    "\n",
    "ALPHABET = [chr(i) for i in range(65, 91)]\n",
    "GRAMMAR_MAP = {}\n",
    "\n",
    "for c in ALPHABET :\n",
    "    GRAMMAR_MAP[c] = ord(c) - ord('A')\n",
    "\n",
    "for i in range(1, 7+1) :\n",
    "    GRAMMAR_MAP[str(i)] = GRAMMAR_MAP['Z'] + 1 + (ord(str(i)) - ord('1'))\n",
    "\n",
    "REVERSE_GRAMMAR_MAP = ['']*len(GRAMMAR_MAP)\n",
    "\n",
    "for char in GRAMMAR_MAP:\n",
    "    REVERSE_GRAMMAR_MAP[GRAMMAR_MAP[char]] = str(char)\n",
    "    \n",
    "json.dumps(GRAMMAR_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input vector will have the shape 8x33.\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "\n",
    "DECODED_PART_LENGTH = SENTENCE_LENGTH - 1\n",
    "output_labels = len(GRAMMAR_MAP) # Number of output labels\n",
    "CHUNK_LENGTH = 1\n",
    "\n",
    "print(\"The input vector will have the shape {}x{}.\"\n",
    "      .format(DECODED_PART_LENGTH//CHUNK_LENGTH, CHUNK_LENGTH*output_labels))\n",
    "\n",
    "parent_dir = '.'\n",
    "COLAB = False\n",
    "if COLAB :\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    parent_dir = '/content/drive/My Drive/LSTM'\n",
    "    \n",
    "train_file = parent_dir+'/'+'data/train.csv'\n",
    "test_file = parent_dir+'/'+'data/answers.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data :  (2543, 8, 33) Training labels :  (2543, 33)\n",
      "Validation data :  (0,) Validation labels :  (0,)\n",
      "Test data :  (379, 8, 33) Test labels :  (379, 33)\n"
     ]
    }
   ],
   "source": [
    "from alienchat_dataset import DataSet\n",
    "alienchat = DataSet(GRAMMAR_MAP, SENTENCE_LENGTH, validation_split=0,chunk_length=CHUNK_LENGTH)\\\n",
    "            .load_data(train_file, test_file)\n",
    "print('Training data : ', alienchat.train.data.shape, 'Training labels : ', alienchat.train.labels.shape)\n",
    "print('Validation data : ', alienchat.validation.data.shape, 'Validation labels : ', alienchat.validation.labels.shape)\n",
    "print('Test data : ', alienchat.test.data.shape, 'Test labels : ', alienchat.test.labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Flatten(input_shape=(DECODED_PART_LENGTH//CHUNK_LENGTH, CHUNK_LENGTH*output_labels)),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(output_labels)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "64/64 [==============================] - 0s 2ms/step - acc: 0.1323 - loss: 3.0961 - val_loss: 2.6210 - val_acc: 0.2122\n",
      "Epoch 2/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.2119 - loss: 2.6147 - val_loss: 2.4683 - val_acc: 0.2554\n",
      "Epoch 3/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.2822 - loss: 2.4273 - val_loss: 2.3344 - val_acc: 0.3281\n",
      "Epoch 4/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.3402 - loss: 2.2946 - val_loss: 2.2488 - val_acc: 0.3399\n",
      "Epoch 5/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.3599 - loss: 2.1821 - val_loss: 2.1714 - val_acc: 0.3674\n",
      "Epoch 6/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.3805 - loss: 2.0787 - val_loss: 2.1426 - val_acc: 0.3752\n",
      "Epoch 7/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.4090 - loss: 2.0013 - val_loss: 2.1168 - val_acc: 0.3851\n",
      "Epoch 8/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.4248 - loss: 1.9565 - val_loss: 2.0876 - val_acc: 0.3792\n",
      "Epoch 9/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.4410 - loss: 1.8838 - val_loss: 2.0635 - val_acc: 0.3831\n",
      "Epoch 10/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.4513 - loss: 1.8282 - val_loss: 2.0627 - val_acc: 0.3949\n",
      "Epoch 11/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.4484 - loss: 1.7821 - val_loss: 2.0697 - val_acc: 0.3870\n",
      "Epoch 12/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.4612 - loss: 1.7620 - val_loss: 2.0516 - val_acc: 0.3929\n",
      "Epoch 13/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.4902 - loss: 1.7106 - val_loss: 2.0496 - val_acc: 0.4008\n",
      "Epoch 14/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.5084 - loss: 1.6501 - val_loss: 2.0328 - val_acc: 0.3890\n",
      "Epoch 15/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.4946 - loss: 1.6284 - val_loss: 2.0327 - val_acc: 0.3910\n",
      "Epoch 16/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.5029 - loss: 1.6253 - val_loss: 2.0392 - val_acc: 0.3831\n",
      "Epoch 17/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.5025 - loss: 1.5830 - val_loss: 2.0353 - val_acc: 0.3792\n",
      "Epoch 18/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.5236 - loss: 1.5390 - val_loss: 2.0351 - val_acc: 0.3929\n",
      "Epoch 19/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.5344 - loss: 1.4917 - val_loss: 2.0407 - val_acc: 0.3949\n",
      "Epoch 20/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.5349 - loss: 1.4762 - val_loss: 2.0498 - val_acc: 0.3929\n",
      "Epoch 21/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.5275 - loss: 1.4705 - val_loss: 2.0591 - val_acc: 0.3929\n",
      "Epoch 22/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.5511 - loss: 1.4277 - val_loss: 2.0670 - val_acc: 0.3831\n",
      "Epoch 23/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.5438 - loss: 1.4351 - val_loss: 2.0564 - val_acc: 0.3969\n",
      "Epoch 24/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.5585 - loss: 1.3788 - val_loss: 2.0653 - val_acc: 0.3929\n",
      "Epoch 25/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.5600 - loss: 1.3631 - val_loss: 2.0650 - val_acc: 0.3969\n",
      "Epoch 26/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.5590 - loss: 1.3518 - val_loss: 2.0568 - val_acc: 0.4028\n",
      "Epoch 27/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.5629 - loss: 1.3465 - val_loss: 2.0539 - val_acc: 0.3831\n",
      "Epoch 28/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.5742 - loss: 1.3113 - val_loss: 2.0972 - val_acc: 0.3792\n",
      "Epoch 29/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.5762 - loss: 1.2832 - val_loss: 2.1104 - val_acc: 0.3969\n",
      "Epoch 30/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.5772 - loss: 1.2986 - val_loss: 2.1093 - val_acc: 0.3811\n",
      "Epoch 31/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.5796 - loss: 1.2751 - val_loss: 2.1221 - val_acc: 0.3772\n",
      "Epoch 32/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.5924 - loss: 1.2440 - val_loss: 2.1295 - val_acc: 0.3969\n",
      "Epoch 33/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.5939 - loss: 1.2399 - val_loss: 2.1465 - val_acc: 0.3910\n",
      "Epoch 34/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.5929 - loss: 1.2035 - val_loss: 2.1452 - val_acc: 0.3831\n",
      "Epoch 35/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.6096 - loss: 1.1755 - val_loss: 2.1645 - val_acc: 0.3890\n",
      "Epoch 36/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.6160 - loss: 1.1632 - val_loss: 2.1736 - val_acc: 0.3870\n",
      "Epoch 37/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.5895 - loss: 1.1801 - val_loss: 2.1735 - val_acc: 0.3988\n",
      "Epoch 38/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.6160 - loss: 1.1515 - val_loss: 2.1798 - val_acc: 0.3988\n",
      "Epoch 39/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.6165 - loss: 1.1569 - val_loss: 2.1887 - val_acc: 0.3831\n",
      "Epoch 40/60\n",
      "64/64 [==============================] - ETA: 0s - acc: 0.6313 - loss: 1.125 - 0s 1ms/step - acc: 0.6347 - loss: 1.1242 - val_loss: 2.1940 - val_acc: 0.3870\n",
      "Epoch 41/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.6352 - loss: 1.0918 - val_loss: 2.1990 - val_acc: 0.3910\n",
      "Epoch 42/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.6357 - loss: 1.0735 - val_loss: 2.2359 - val_acc: 0.3949\n",
      "Epoch 43/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.6386 - loss: 1.0757 - val_loss: 2.2450 - val_acc: 0.3929\n",
      "Epoch 44/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.6490 - loss: 1.0584 - val_loss: 2.2485 - val_acc: 0.3969\n",
      "Epoch 45/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.6627 - loss: 1.0488 - val_loss: 2.2620 - val_acc: 0.3890\n",
      "Epoch 46/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.6519 - loss: 1.0520 - val_loss: 2.2930 - val_acc: 0.3811\n",
      "Epoch 47/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.6411 - loss: 1.0484 - val_loss: 2.3119 - val_acc: 0.3811\n",
      "Epoch 48/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.6445 - loss: 1.0256 - val_loss: 2.3184 - val_acc: 0.3831\n",
      "Epoch 49/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.6529 - loss: 1.0177 - val_loss: 2.2962 - val_acc: 0.3831\n",
      "Epoch 50/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.6657 - loss: 1.0110 - val_loss: 2.3176 - val_acc: 0.3713\n",
      "Epoch 51/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.6657 - loss: 0.9808 - val_loss: 2.3502 - val_acc: 0.3890\n",
      "Epoch 52/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.6613 - loss: 0.9707 - val_loss: 2.3834 - val_acc: 0.3713\n",
      "Epoch 53/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.6706 - loss: 0.9694 - val_loss: 2.3736 - val_acc: 0.3792\n",
      "Epoch 54/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.6647 - loss: 0.9735 - val_loss: 2.4072 - val_acc: 0.3890\n",
      "Epoch 55/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.6770 - loss: 0.9637 - val_loss: 2.3897 - val_acc: 0.3929\n",
      "Epoch 56/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.6750 - loss: 0.9541 - val_loss: 2.4033 - val_acc: 0.3752\n",
      "Epoch 57/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.6898 - loss: 0.9068 - val_loss: 2.4502 - val_acc: 0.3772\n",
      "Epoch 58/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.6878 - loss: 0.9076 - val_loss: 2.4568 - val_acc: 0.3752\n",
      "Epoch 59/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.6844 - loss: 0.9025 - val_loss: 2.4700 - val_acc: 0.3752\n",
      "Epoch 60/60\n",
      "64/64 [==============================] - 0s 1ms/step - acc: 0.6814 - loss: 0.9274 - val_loss: 2.4573 - val_acc: 0.3772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5d4c259630>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(alienchat.train.data, alienchat.train.labels, epochs=60, validation_split=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 0s - acc: 0.7230 - loss: 0.8941\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(alienchat.test.data, alienchat.test.labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def secondmax(arr) :\n",
    "    b = np.sort(arr)\n",
    "    return np.where(arr == b[-2])[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>predicted_word</th>\n",
       "      <th>encoded_word</th>\n",
       "      <th>second_best_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2222222M2</td>\n",
       "      <td>M</td>\n",
       "      <td>2</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>R2SR2SR22</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>62MSRSSSR</td>\n",
       "      <td>E</td>\n",
       "      <td>R</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2WKM1S2SR</td>\n",
       "      <td>S</td>\n",
       "      <td>R</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>SR22SR2SR</td>\n",
       "      <td>2</td>\n",
       "      <td>R</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6YK2LLLZT</td>\n",
       "      <td>S</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>E22MMZEHP</td>\n",
       "      <td>5</td>\n",
       "      <td>P</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>SRSSGSRSR</td>\n",
       "      <td>2</td>\n",
       "      <td>R</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>6S2S2SEHP</td>\n",
       "      <td>5</td>\n",
       "      <td>P</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2G2GSS1SR</td>\n",
       "      <td>S</td>\n",
       "      <td>R</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence predicted_word encoded_word second_best_pred\n",
       "2   2222222M2              M            2                S\n",
       "10  R2SR2SR22              S            2                2\n",
       "13  62MSRSSSR              E            R                R\n",
       "14  2WKM1S2SR              S            R                G\n",
       "18  SR22SR2SR              2            R                R\n",
       "20  6YK2LLLZT              S            T                T\n",
       "32  E22MMZEHP              5            P                P\n",
       "35  SRSSGSRSR              2            R                R\n",
       "38  6S2S2SEHP              5            P                P\n",
       "42  2G2GSS1SR              S            R                G"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = alienchat.test.rawdata\n",
    "predictions = model.predict(alienchat.test.data)\n",
    "test[\"predicted_word\"] = [REVERSE_GRAMMAR_MAP[np.argmax(prediction)] for prediction in predictions]\n",
    "test[\"second_best_pred\"] = [REVERSE_GRAMMAR_MAP[secondmax(prediction)] for prediction in predictions]\n",
    "#print(validate['predicted_word'])\n",
    "test[\"encoded_word\"] = test[\"sentence\"].apply(lambda x : x[-1])\n",
    "test[[\"sentence\",\"predicted_word\",\"encoded_word\", \"second_best_pred\"]][test[\"encoded_word\"] != test['predicted_word']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
