{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a0SL8RBkpwE1"
   },
   "source": [
    "# Predicting next word in a sentence \n",
    "## word here is represented by a character\n",
    "Dataset is not published here, but for an idea, training is done on sentences that look like <b>AEDF234TH</b> and testing is done on sentences like <b>ASE452DG?</b>.\n",
    "\n",
    "This is a standard LSTM implementation of this problem, using [Keras](https://keras.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GViJckmYpwE4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "np.random.seed(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem-specific data parameters\n",
    "Each sentence in training data are of length :  `SENTENCE_LENGTH=9`\n",
    "\n",
    "The vocabulary, `GRAMMAR` (or `GRAMMAR_MAP`, if the frequency of each word are taken into account) is given as :\n",
    "\n",
    "`[A-Z][1-7]`\n",
    "\n",
    "The prediction on the test data is the missing last word. So, in all the data sets, the **last** character in every sentence is taken as the `label` and the rest of it as `training` data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "JqeyHfWVpwFG",
    "outputId": "2b7c6927-3178-459d-e249-10d51d274f73"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"H\": 7, \"D\": 3, \"I\": 8, \"U\": 20, \"W\": 22, \"N\": 13, \"G\": 6, \"5\": 30, \"Q\": 16, \"1\": 26, \"V\": 21, \"4\": 29, \"3\": 28, \"J\": 9, \"O\": 14, \"K\": 10, \"Y\": 24, \"R\": 17, \"7\": 32, \"P\": 15, \"A\": 0, \"T\": 19, \"C\": 2, \"F\": 5, \"M\": 12, \"B\": 1, \"2\": 27, \"6\": 31, \"Z\": 25, \"X\": 23, \"S\": 18, \"E\": 4, \"L\": 11}'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Utils for this problem\n",
    "\n",
    "SENTENCE_LENGTH = 9\n",
    "\n",
    "ALPHABET = [chr(i) for i in range(65, 91)]\n",
    "GRAMMAR_MAP = {}\n",
    "\n",
    "for c in ALPHABET :\n",
    "    GRAMMAR_MAP[c] = ord(c) - ord('A')\n",
    "\n",
    "for i in range(1, 7+1) :\n",
    "    GRAMMAR_MAP[str(i)] = GRAMMAR_MAP['Z'] + 1 + (ord(str(i)) - ord('1'))\n",
    "\n",
    "REVERSE_GRAMMAR_MAP = ['']*len(GRAMMAR_MAP)\n",
    "\n",
    "for char in GRAMMAR_MAP:\n",
    "    REVERSE_GRAMMAR_MAP[GRAMMAR_MAP[char]] = str(char)\n",
    "    \n",
    "json.dumps(GRAMMAR_MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CIIily-vpwFg"
   },
   "source": [
    "## Preprocessing the data\n",
    "\n",
    "First, we will convert every sentence into a vector using One-Hot Encoding. \n",
    "Here, each word in the sentence is represented by a vector of n binary sub-vectors, where n is the number of different chars in the specified GRAMMAR (33, 26 alphabet + 7 numbers). \n",
    "\n",
    "Example:<br>\n",
    "A becomes:<br>[**1**, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "1 becomes:<br>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, **1**, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "_ABCD1234_ becomes:<br>\n",
    "[[<b>1</b>, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],<br>\n",
    "  [0, <b>1</b>, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],<br>\n",
    " [0, 0, <b>1</b>, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],<br>\n",
    " [0, 0, 0, <b>1</b>, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],<br>\n",
    " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, <b>1</b>, 0, 0, 0, 0, 0, 0,],<br>\n",
    " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, <b>1</b>, 0, 0, 0, 0, 0],<br>\n",
    " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, <b>1</b>, 0, 0, 0, 0],<br>\n",
    " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..., 0, 0, 0, 0, 0, 0, 0, 0, <b>1</b>, 0, 0, 0]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although not completely explored, one of the possibilities considered is to chunk the input vector into small groups. That is a sentence `ABCD1234`, if by default seen as collection of \n",
    "\n",
    "chunks of `1` word is represented as `[A, B, C, D, 1, 2, 3, 4]`\n",
    "\n",
    "and the corresponding one-hot vector encoding will be of shape : `(8, 33)`\n",
    "\n",
    "whereas if chunks of `2` words are considered, the sentence is represented as `[AB, CD, 12, 34]`\n",
    "\n",
    "and the corresponding one-hot vector encoding will be of shape : `(4, (2,33))` or `(4, 66)` by flattening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Yiqu7SxOpwFh",
    "outputId": "d76fdaea-51cf-48fe-fbef-cec3646138aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input vector will have the shape 8x33.\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "\n",
    "DECODED_PART_LENGTH = SENTENCE_LENGTH - 1\n",
    "output_labels = len(GRAMMAR_MAP) # Number of output labels\n",
    "CHUNK_LENGTH = 1\n",
    "\n",
    "print(\"The input vector will have the shape {}x{}.\"\n",
    "      .format(DECODED_PART_LENGTH//CHUNK_LENGTH, CHUNK_LENGTH*output_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data location\n",
    "If using Google colab, set `COLAB=True` below and update the `parent_dir` to the parent folder where the folder `data` is present.\n",
    "\n",
    "BY default, the code runs locally and `parent_dir` is set to the **current** directory.\n",
    "\n",
    "The `data` folder is expected to have atleast 2 files : `train.csv` and `answers.csv`\n",
    "\n",
    "```\n",
    "parent_dir\n",
    "    |\n",
    "    |\n",
    "    --data\n",
    "        |\n",
    "        -- train.csv\n",
    "        -- answers.csv      \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = '.'\n",
    "COLAB = False\n",
    "if COLAB :\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    parent_dir = '/content/drive/My Drive/LSTM'\n",
    "    \n",
    "train_file = parent_dir+'/'+'data/train.csv'\n",
    "test_file = parent_dir+'/'+'data/answers.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation library\n",
    "A small library named `alienchat_dataset` is written to prepare the datasets of **train**, **validation (fixed : 20%)** and **test** from the provided data files. \n",
    "\n",
    "`alienchat_dataset` provides a dataset object `DataSet` which is initialized by providing the vocabulary and the length of each sentence (including the label).\n",
    "\n",
    "Although it is just preparing respective one-hot encoding as described above, some experimental features are present in the library such as \n",
    "```\n",
    "chunk_length (default : 1)\n",
    "```\n",
    "\n",
    "`DataSet` has a method `load_data` which expects the `train_file` and `test_file`, as described above.\n",
    "\n",
    "There are some (experimental) optional parameters that can be provided to `load_data` like :\n",
    "\n",
    "`blow_training_data (default : 0)` - to repeat the training data and shuffle, if it is really small\n",
    "\n",
    "`patterns (default : False)` - when set to `True`, for every `sentence` of length `SENTENCE_LENGTH`, a sub-sentence of length (`1+SENTENCE_LENGTH//2`) is added. \n",
    "\n",
    "The idea of `patterns` is to see if we can learn from subsequences for some repeating grammar. The idea is not completely explored.\n",
    "\n",
    "When used, the input vector is filled with `np.zeros(len(GRAMMAR))` for every word less than the `SENTENCE_LENGTH`, in this case : (`SENTENCE_LENGTH - (1+SENTENCE_LENGTH//2)`) \n",
    "\n",
    "The data set preparation can be explored here : [AlienChatDataSet.ipynb](AlienChatDataSet.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VYe3uSHJpwFq"
   },
   "outputs": [],
   "source": [
    "from alienchat_dataset import DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AhQeeeXipwFu"
   },
   "outputs": [],
   "source": [
    "alienchat = DataSet(GRAMMAR_MAP, SENTENCE_LENGTH, chunk_length=CHUNK_LENGTH)\\\n",
    "            .load_data(train_file, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "UMuf1K3ApwFz",
    "outputId": "6f18c7dd-f79a-48e4-f29d-c3e341c48cde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data :  (2034, 8, 33) Training labels :  (2034, 33)\n",
      "Validation data :  (509, 8, 33) Validation labels :  (509, 33)\n",
      "Test data :  (379, 8, 33) Test labels :  (379, 33)\n"
     ]
    }
   ],
   "source": [
    "print('Training data : ', alienchat.train.data.shape, 'Training labels : ', alienchat.train.labels.shape)\n",
    "print('Validation data : ', alienchat.validation.data.shape, 'Validation labels : ', alienchat.validation.labels.shape)\n",
    "print('Test data : ', alienchat.test.data.shape, 'Test labels : ', alienchat.test.labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2t3gWsvnpwF9"
   },
   "source": [
    "## Building, training and evaluating the model\n",
    "\n",
    "### Architecture\n",
    "Sequential, at least 1 LSTM layer, Final Dense layer with Activation (Sigmoid)\n",
    "\n",
    "**Parameter** `num_extra_hidden_layers` adds specified number of extra LSTM layers in between the first LSTM layer and the Dense layer.\n",
    "#### Dropout\n",
    "At least one Dropout layer for the last LSTM layer. Explored a bit with Dropout layers for each of the LSTM layers, if there were more.\n",
    "\n",
    "**Number of neurons in each hidden layers** :\n",
    "As a starting point, the following [formula](https://stats.stackexchange.com/a/136542) is referred :\n",
    "\n",
    "$$N_h = \\frac{N_s} {(\\alpha * (N_i + N_o))}$$\n",
    "\n",
    "$N_i$ is the number of input neurons, $N_o$ the number of output neurons, $N_s$ the number of samples in the trainings data, and $\\alpha$ represents a scaling factor that is usually between 2 and 10. \n",
    "\n",
    "Alternatively, another simple rule, not necessarily optimal :\n",
    "\n",
    "$$N_h = \\frac{2} {3} *(N_i + N_o)$$\n",
    "\n",
    "### Loss function\n",
    "`categorical_crossentropy` : As mentioned [here](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy), it is suitable *\"when there are two or more label classes\"*\n",
    "#### Metrics\n",
    "As of [TF2.2+](https://github.com/tensorflow/tensorflow/blob/fabcd8f89cd5975331994049705e15cb75f32e0c/tensorflow/python/keras/engine/training.py#L463) documentation, using the string `acc` metrics automatically selects the relevant accuracy suitable to the loss function used. \n",
    "### Optimizer\n",
    "`Adam` : variant of classic SGD, first described [here](https://arxiv.org/abs/1412.6980) but commonly known to adaptively vary learning rates.\n",
    "\n",
    "### Summary\n",
    "\n",
    "The best `test` accuracy of the model achieved as the following : `\n",
    "\n",
    "| Extra Hidden layers      | Neurons / Hidden Layer | Epochs     | Train (loss / accuracy)| Validation (loss / accuracy)| Test (loss / accuracy)|\n",
    "| :---        |    :----:   |    :----:   |    :----:   |    :----:   |    :----:   |\n",
    "| 1      | Low (alpha=1)       | Moderate (50)   | 2.1047 / 34.96% | 2.0883 / 32.61% | 1.0338 / <b>88.13%</b>|\n",
    "\n",
    "\n",
    "The best `train` accuracy of the model achieved, with a poor validation accuracy and lower `test` accuracy, as the following : `\n",
    "\n",
    "| Extra Hidden layers      | Neurons / Hidden Layer | Epochs     | Train (loss / accuracy)| Validation (loss / accuracy)| Test (loss / accuracy)|\n",
    "| :---        |    :----:   |    :----:   |    :----:   |    :----:   |    :----:   |\n",
    "| 1      | Moderate (alpha=.5)       | High (100)   | 0.8386 / **70.35%** | 2.3840 / 37.33% | 0.9158 / <b>73.09%</b>|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "GkAMgMfSpwF-",
    "outputId": "4a8f6342-1267-4ab1-c942-dc19317be901"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "The number of hidden nodes is 49.\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "print('Build model...')\n",
    "#hidden_nodes = int(2/3 * (DECODED_PART_LENGTH * output_labels))\n",
    "\n",
    "alpha = 1 # to 8\n",
    "\n",
    "hidden_nodes = int(alienchat.train.data.shape[0] / (alpha * (DECODED_PART_LENGTH + output_labels)))\n",
    "\n",
    "print(\"The number of hidden nodes is {}.\".format(hidden_nodes))\n",
    "num_extra_hidden_layers = 1\n",
    "model = Sequential()\n",
    "model.add(LSTM(hidden_nodes, return_sequences=(num_extra_hidden_layers>0), input_shape=(DECODED_PART_LENGTH//CHUNK_LENGTH, CHUNK_LENGTH*output_labels)))\n",
    "model.add(Dropout(0.3))\n",
    "if num_extra_hidden_layers > 0 :\n",
    "    for _ in range(num_extra_hidden_layers-1) :\n",
    "        model.add(LSTM(hidden_nodes, return_sequences=True))\n",
    "        model.add(Dropout(0.3))\n",
    "    model.add(LSTM(hidden_nodes))\n",
    "    model.add(Dropout(0.3))\n",
    "model.add(Dense(units=output_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "igWU764ApwGD",
    "outputId": "1e0b22e9-eabc-4ba3-a9b9-748d24902270"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 3.2737 - acc: 0.1391 - val_loss: 2.7948 - val_acc: 0.1670\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 2.8222 - acc: 0.1421 - val_loss: 2.6713 - val_acc: 0.1670\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 2.7194 - acc: 0.1613 - val_loss: 2.6176 - val_acc: 0.1670\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 2.6583 - acc: 0.1608 - val_loss: 2.5602 - val_acc: 0.2200\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 2.6017 - acc: 0.1834 - val_loss: 2.5083 - val_acc: 0.2181\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 2.5590 - acc: 0.2080 - val_loss: 2.4583 - val_acc: 0.2240\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 2.5155 - acc: 0.2050 - val_loss: 2.4217 - val_acc: 0.2299\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 2.4995 - acc: 0.2291 - val_loss: 2.4090 - val_acc: 0.2515\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 2.4413 - acc: 0.2409 - val_loss: 2.4004 - val_acc: 0.2475\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 2.4273 - acc: 0.2532 - val_loss: 2.3607 - val_acc: 0.2613\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 2.4081 - acc: 0.2380 - val_loss: 2.3213 - val_acc: 0.2750\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 2.3732 - acc: 0.2517 - val_loss: 2.3045 - val_acc: 0.2849\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 2.3467 - acc: 0.2532 - val_loss: 2.2837 - val_acc: 0.2868\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 2.3311 - acc: 0.2586 - val_loss: 2.2662 - val_acc: 0.2908\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 2.2991 - acc: 0.2689 - val_loss: 2.2561 - val_acc: 0.2809\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 2.2830 - acc: 0.2748 - val_loss: 2.2305 - val_acc: 0.3026\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 2.2557 - acc: 0.2832 - val_loss: 2.2482 - val_acc: 0.2908\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 2.2339 - acc: 0.2748 - val_loss: 2.2087 - val_acc: 0.2790\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 2.2181 - acc: 0.2920 - val_loss: 2.1912 - val_acc: 0.2986\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 2.2149 - acc: 0.3043 - val_loss: 2.1827 - val_acc: 0.2809\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 2.1945 - acc: 0.2940 - val_loss: 2.1900 - val_acc: 0.3006\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 2.1631 - acc: 0.3004 - val_loss: 2.1581 - val_acc: 0.2947\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 2.1444 - acc: 0.3201 - val_loss: 2.1340 - val_acc: 0.3084\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 2.1522 - acc: 0.3156 - val_loss: 2.1316 - val_acc: 0.3183\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 2.1288 - acc: 0.3201 - val_loss: 2.1425 - val_acc: 0.3084\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 2.1000 - acc: 0.3235 - val_loss: 2.1091 - val_acc: 0.3242\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 2.0743 - acc: 0.3446 - val_loss: 2.1180 - val_acc: 0.3281\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 2.0624 - acc: 0.3402 - val_loss: 2.0965 - val_acc: 0.3124\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 2.0644 - acc: 0.3397 - val_loss: 2.0930 - val_acc: 0.3418\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 2.0374 - acc: 0.3599 - val_loss: 2.0953 - val_acc: 0.3438\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 2.0170 - acc: 0.3584 - val_loss: 2.0721 - val_acc: 0.3536\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 2.0320 - acc: 0.3515 - val_loss: 2.0569 - val_acc: 0.3497\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 1.9924 - acc: 0.3584 - val_loss: 2.0494 - val_acc: 0.3595\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 1.9726 - acc: 0.3781 - val_loss: 2.0492 - val_acc: 0.3556\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 1.9828 - acc: 0.3727 - val_loss: 2.0365 - val_acc: 0.3556\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 1.9640 - acc: 0.3736 - val_loss: 2.0420 - val_acc: 0.3615\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 1.9275 - acc: 0.3736 - val_loss: 2.0379 - val_acc: 0.3595\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.9456 - acc: 0.3682 - val_loss: 2.0227 - val_acc: 0.3694\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1.9330 - acc: 0.3879 - val_loss: 2.0297 - val_acc: 0.3654\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 1.8967 - acc: 0.3963 - val_loss: 2.0259 - val_acc: 0.3615\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 1.9000 - acc: 0.3899 - val_loss: 2.0403 - val_acc: 0.3497\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 1.8895 - acc: 0.4022 - val_loss: 2.0205 - val_acc: 0.3595\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 1.8874 - acc: 0.3884 - val_loss: 2.0155 - val_acc: 0.3752\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 1.8835 - acc: 0.3987 - val_loss: 2.0125 - val_acc: 0.3674\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 1.8705 - acc: 0.3977 - val_loss: 1.9986 - val_acc: 0.3752\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 1.8572 - acc: 0.4066 - val_loss: 1.9908 - val_acc: 0.3831\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 1.8318 - acc: 0.4120 - val_loss: 1.9885 - val_acc: 0.3831\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 1.8340 - acc: 0.4154 - val_loss: 2.0058 - val_acc: 0.3536\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 1.8151 - acc: 0.4184 - val_loss: 2.0055 - val_acc: 0.3694\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 1.8083 - acc: 0.4238 - val_loss: 1.9940 - val_acc: 0.3870\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_epochs=50\n",
    "train_x = alienchat.train.data\n",
    "train_y = alienchat.train.labels\n",
    "validate_x = alienchat.validation.data\n",
    "validate_y = alienchat.validation.labels\n",
    "#_ = model.fit(train_x, train_y, batch_size=batch_size, epochs=num_epochs, validation_split=0.2)\n",
    "_ = model.fit(train_x, train_y, batch_size=batch_size, epochs=num_epochs, validation_data=(validate_x, validate_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "mGnGpYPvpwGJ",
    "outputId": "a990ad2b-87c3-4205-bc34-d17236307f1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 - 0s - loss: 0.8571 - acc: 0.8760\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(alienchat.test.data, alienchat.test.labels, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curious about the wrongly predicted cases, the second best prediction is compared to the actual label. Turns out, from the first glimpse, the second best prediction was quite often the correct label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def secondmax(arr) :\n",
    "    b = np.sort(arr)\n",
    "    return np.where(arr == b[-2])[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "id": "mo7h3oREpwGP",
    "outputId": "5b045665-511d-4719-fff1-4a2aed7ce71f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>predicted_word</th>\n",
       "      <th>encoded_word</th>\n",
       "      <th>second_best_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2ESSE12S1</td>\n",
       "      <td>E</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6MSSSG2M2</td>\n",
       "      <td>M</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6YK2LLLZT</td>\n",
       "      <td>I</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2G2GSS1SR</td>\n",
       "      <td>S</td>\n",
       "      <td>R</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>SRSRSRS4R</td>\n",
       "      <td>P</td>\n",
       "      <td>R</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>GRS2GRSGR</td>\n",
       "      <td>G</td>\n",
       "      <td>R</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>MMMMSSSSR</td>\n",
       "      <td>S</td>\n",
       "      <td>R</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>K2S1SEWEP</td>\n",
       "      <td>R</td>\n",
       "      <td>P</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>SM2SKZESP</td>\n",
       "      <td>E</td>\n",
       "      <td>P</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>SRSRSG2M2</td>\n",
       "      <td>M</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence predicted_word encoded_word second_best_pred\n",
       "15  2ESSE12S1              E            1                1\n",
       "16  6MSSSG2M2              M            2                2\n",
       "20  6YK2LLLZT              I            T                T\n",
       "42  2G2GSS1SR              S            R                R\n",
       "45  SRSRSRS4R              P            R                R\n",
       "51  GRS2GRSGR              G            R                R\n",
       "65  MMMMSSSSR              S            R                R\n",
       "67  K2S1SEWEP              R            P                P\n",
       "77  SM2SKZESP              E            P                S\n",
       "78  SRSRSG2M2              M            2                2"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = alienchat.test.rawdata\n",
    "predictions = model.predict(alienchat.test.data)\n",
    "test[\"predicted_word\"] = [REVERSE_GRAMMAR_MAP[np.argmax(prediction)] for prediction in predictions]\n",
    "test[\"second_best_pred\"] = [REVERSE_GRAMMAR_MAP[secondmax(prediction)] for prediction in predictions]\n",
    "#print(validate['predicted_word'])\n",
    "test[\"encoded_word\"] = test[\"sentence\"].apply(lambda x : x[-1])\n",
    "test[[\"sentence\",\"predicted_word\",\"encoded_word\", \"second_best_pred\"]][test[\"encoded_word\"] != test['predicted_word']].head(10)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AlienChat_LSTM_Keras.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
